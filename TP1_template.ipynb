{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajo Practico 1\n",
    "\n",
    "## Integrantes:\n",
    "Martin Leon Cuadrado Bertollo : 824/23\n",
    "\n",
    "Segundo Sacchi : 434/23\n",
    "\n",
    "Lucas Petersen Frers : 1149/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de paquetes necesarios para graficar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd # Para leer archivos\n",
    "import geopandas as gpd # Para hacer cosas geográficas\n",
    "import seaborn as sns # Para hacer plots lindos\n",
    "import networkx as nx # Construcción de la red en NetworkX\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preambulo\n",
    "\n",
    "En esta sección cargamos los datos y los visualizamos. También construimos la matriz de adyacencia de la red de museos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos de los museos\n",
    "\n",
    "El listado de los museos, con el que se construye el [mapa](https://mapas.museosabiertos.org/museos/caba/), lo podemos encontrar [acá](https://github.com/MuseosAbiertos/Leaflet-museums-OpenStreetMap/blob/principal/data/export.geojson?short_path=bc357f3). También descargamos los barrios de CABA como complemento para los gráficos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos el archivo, retenemos aquellos museos que están en CABA, y descartamos aquellos que no tienen latitud y longitud\n",
    "museos = gpd.read_file('https://raw.githubusercontent.com/MuseosAbiertos/Leaflet-museums-OpenStreetMap/refs/heads/principal/data/export.geojson')\n",
    "barrios = gpd.read_file('https://cdn.buenosaires.gob.ar/datosabiertos/datasets/ministerio-de-educacion/barrios/barrios.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En esta línea:\n",
    "# Tomamos museos, lo convertimos al sistema de coordenadas de interés, extraemos su geometría (los puntos del mapa), \n",
    "# calculamos sus distancias a los otros puntos de df, redondeamos (obteniendo distancia en metros), y lo convertimos a un array 2D de numpy\n",
    "D = museos.to_crs(\"EPSG:22184\").geometry.apply(lambda g: museos.to_crs(\"EPSG:22184\").distance(g)).round().to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de adyacencia: construimos una matriz conectando a cada museo con los $m$ más cercanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construye_adyacencia(D,m): \n",
    "    # Función que construye la matriz de adyacencia del grafo de museos\n",
    "    # D matriz de distancias, m cantidad de links por nodo\n",
    "    # Retorna la matriz de adyacencia como un numpy.\n",
    "    D = D.copy()\n",
    "    l = [] # Lista para guardar las filas\n",
    "    for fila in D: # recorriendo las filas, anexamos vectores lógicos\n",
    "        l.append(fila<=fila[np.argsort(fila)[m]] ) # En realidad, elegimos todos los nodos que estén a una distancia menor o igual a la del m-esimo más cercano\n",
    "    A = np.asarray(l).astype(int) # Convertimos a entero\n",
    "    np.fill_diagonal(A,0) # Borramos diagonal para eliminar autolinks\n",
    "    return(A)\n",
    "\n",
    "m = 3 # Cantidad de links por nodo\n",
    "A = construye_adyacencia(D,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción de la red en NetworkX (sólo para las visualizaciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolución del TP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Muestre que el vector de rankings $p$ es solución de la ecuación\n",
    "$M \\cdot p = b$, con $M = \\frac{N}{\\alpha} \\cdot (I - (1 - \\alpha) \\cdot C)$ y $b$ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partamos de la ecuacion siguiente para llegar hacia $M \\cdot p = b$:\n",
    "\n",
    "$$\n",
    "p = (1 - \\alpha) \\cdot C \\cdot p + \\frac{\\alpha}{N} \\cdot 1_v\n",
    "$$\n",
    "$$\n",
    "0 = (1 - \\alpha) \\cdot C \\cdot p - p + \\frac{\\alpha}{N} \\cdot 1_v\n",
    "$$\n",
    "$$\n",
    "- \\frac{\\alpha}{N} \\cdot 1_v = (1 - \\alpha) \\cdot C \\cdot p - p\n",
    "$$\n",
    "Aca sacamos factor comun usando $p$ \n",
    "$$\n",
    "- \\frac{\\alpha}{N} \\cdot 1_v = ((1 - \\alpha) \\cdot C - I) \\cdot p \n",
    "$$\n",
    "$$\n",
    "\\frac{\\alpha}{N} \\cdot 1_v = -((1 - \\alpha) \\cdot C - I) \\cdot p \n",
    "$$\n",
    "$$\n",
    "\\frac{\\alpha}{N} \\cdot 1_v = (I - (1 - \\alpha) \\cdot C) \\cdot p \n",
    "$$\n",
    "$$\n",
    "1_v = \\frac{N}{\\alpha} \\cdot (I - (1 - \\alpha) \\cdot C) \\cdot p   \n",
    "$$\n",
    "Por la definicion de $M$ simplemente reemplazamos y llegamos a:\n",
    "$$\n",
    "1_v = M \\cdot p   \n",
    "$$\n",
    "Entonces llegamos que el vector de rankings $p$ es el resultado de $M \\cdot p = b$ con $b = 1_v$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 2:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ¿Qué condiciones se deben cumplir para que exista una única solución a la ecuación del\n",
    "punto anterior?\n",
    ">\n",
    "\n",
    " Para que $M \\cdot p = b$ tenga una sola solucion las columnas de $M$ deberian ser linealmente independientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ¿Se cumplen estas condiciones para la matriz M tal como fue construida\n",
    "para los museos, cuando 0 < α < 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Sabemos que:\n",
    "$$\n",
    "M = \\frac{N}{\\alpha} \\cdot (I - (1 - \\alpha) \\cdot C)\n",
    "$$\n",
    "Donde $\\frac{N}{\\alpha}$ es un escalar, $N$ es el tamaño de $C$ y $\\alpha$ un numero mayor 0 y menor estricto que 1, por lo que verdaderamente nos interesa ver para convencernos de que la matriz $M$ tiene solucion unica es la parte de:\n",
    "$$\n",
    "(I - (1 - \\alpha) \\cdot C)\n",
    "$$\n",
    "Lo que sabemos de esta parte de la formula es que (por la definicion de dada anteriormente)\n",
    "$$\n",
    "0 < (1 - \\alpha) < 1\n",
    "$$\n",
    "\n",
    "Luego, sobre $C$ sabemos que es una matriz estocastica, esto quiere decir que:\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{n} C_{ij} = 1 \\quad \\forall\\, i \\in \\{1, \\dots, n\\} \\quad \\land \\quad C_{ij} \\geq 0 \\quad \\forall\\, i, j \\in \\{1, \\dots, n\\}\n",
    "$$\n",
    "\n",
    "Entonces,\n",
    "$$\n",
    "0 \\leq C_{ij} \\leq 1 \\quad \\forall\\, i, j \\in \\mathbb{Z},\\ 1 \\leq i,j \\leq n\n",
    "$$\n",
    "\n",
    "Ahora bien, teniendo esta informacion como podriamos demostrar que la Matriz $M$ tiene solucion unica?\n",
    "\n",
    "Sea $B \\in \\mathbb{R}^{n \\times n}$ tal que...\n",
    "$$\n",
    "B = (1-\\alpha) \\cdot C \\\\\n",
    "$$\n",
    "Puedo deducir que\n",
    "$$\n",
    "\\text{como} \\quad 0 < (1 - \\alpha) < 1 \\quad \\land \\quad \\sum_{j=1}^{n} C_{ij} = 1 \\quad \\forall\\, i \\in \\{1, \\dots, n\\} \\quad \\Rightarrow \\sum_{j=1}^{n} B_{ij} < 1 \\quad \\forall\\, i \\in \\{1, \\dots, n\\} \n",
    "$$\n",
    "\n",
    "Luego,\n",
    "$$\n",
    "\\|\\mathbf{B}\\|_1 < 1\n",
    "$$\n",
    "Teniendo esto en cuenta, tratemos de definir por absurdo que el resultado de la $(I - B)$ es inversible, es decir, que es sus columnas son linealmente independientes y por lo tanto existe un único resultado para $M \\cdot p = b$. \n",
    "\n",
    "Supongamos que el resultado de dicha resta NO tiene columnas linealmente independientes, esto significa que\n",
    "\n",
    "$$\n",
    "\\dim\\left( \\text{Nu}(I - B) \\right) > 0\n",
    "$$\n",
    "\n",
    "pues tendremos valores ademas del $x = 0$ para los cuales:\n",
    "\n",
    "$$\n",
    "(I - B) \\cdot \\mathbf{x} = \\mathbf{0} \n",
    "$$\n",
    "Por lo tanto podemos asegurar que existe un $x != 0$ para el cual el resultado de dicha ecuacion es 0. Por lo cual podemos plantear que:\n",
    "$$\n",
    "(I - B)\\cdot \\mathbf{x} = \\mathbf{0} \n",
    "$$ \n",
    "$$\n",
    "I\\cdot \\mathbf{x} - B\\cdot \\mathbf{x} = 0 \n",
    "$$ \n",
    "$$\n",
    "\\mathbf{x} - B\\cdot \\mathbf{x} = 0\n",
    "$$\n",
    "$$\n",
    "\\mathbf{x} = B\\cdot \\mathbf{x}\n",
    "$$\n",
    "Y terminamos llegando a una igualdad:\n",
    "$$\n",
    "\\|\\mathbf{x}\\|_1 = \\|\\mathbf{Bx}\\|_1 \n",
    "$$\n",
    "Por propiedad de normas, podemos definir que:\n",
    "$$\n",
    "\\|\\mathbf{Bx}\\|_1 \\leq \\|\\mathbf{B}\\|_1 \\cdot \\|\\mathbf{x}\\|_1 \n",
    "$$\n",
    "Y ahora reemplazamos\n",
    "$$\n",
    "\\|\\mathbf{x}\\|_1 \\leq \\|\\mathbf{B}\\|_1 \\cdot \\|\\mathbf{x}\\|_1 \n",
    "$$\n",
    "$$\n",
    "1 \\leq \\|\\mathbf{B}\\|_1\n",
    "$$\n",
    "ABSURDO! sabiamos que $\\|\\mathbf{B}\\|_1 < 1$, este error se debe en que asumimos que $(I - B)$ no era inversible, por lo que al demostrar que la matriz resultado de esta resta es inversible, podemos asegurar que $M$ es inversible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Encuentre el vector $p = M^{-1} \\cdot b$ en los siguientes casos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seteamos las variables que sabemos que son globales y no cambian\n",
    "N = A.shape[0]\n",
    "I = np.eye(N, k=0)\n",
    "\n",
    "#Funciones que utilizamos\n",
    "\n",
    "\n",
    "# Aca realizamos la factorizacion (sin pivoteo) de una matriz cuadrada haciendo las transformaciones fila a fila llenando de 0's \n",
    "# debajo de la diagonal, y luego guardando allí la iteración de L_i, este se estará corriendo para guardar en la misma matriz\n",
    "# el L y el U. Luego se separan ambas triangulaciones superiores e inferiores y se retornan\n",
    "\n",
    "def elim_gaussiana(A):\n",
    "    m=A.shape[0]\n",
    "    n=A.shape[1]\n",
    "    Ac = A.copy()\n",
    "    \n",
    "    if m!=n:\n",
    "        print('Matriz no cuadrada')\n",
    "        return\n",
    "    \n",
    "    for iteracion in range(n - 1):\n",
    "        for fila in range(iteracion + 1, n):\n",
    "            divisor = Ac[fila][iteracion] / A[iteracion][iteracion]\n",
    "            Ac[fila][iteracion] = divisor  \n",
    "            \n",
    "            for columna in range(iteracion + 1, n):  \n",
    "                Ac[fila][columna] = Ac[fila][columna] - divisor * Ac[iteracion][columna]\n",
    "            \n",
    "    L = np.tril(Ac,-1) + np.eye(n)\n",
    "    U = np.triu(Ac)\n",
    "    \n",
    "    return L, U\n",
    "\n",
    "# Resuelve L*y = b, con L siendo matriz triangular inferior\n",
    "\n",
    "def y_calculator(L, b):\n",
    "    return scipy.linalg.solve_triangular(L, b, lower=True)\n",
    "\n",
    "# Resuelve U*x = y, con U siendo matriz triangular superior\n",
    "\n",
    "def x_calculator(U, y):\n",
    "    return scipy.linalg.solve_triangular(U, y, lower=False)\n",
    "\n",
    "\n",
    "def matriz_M(a,m):\n",
    "    # Armamos K que es la identidad multiplicada por m. Como necesitamos la inversa solamente calculamos la inversa de la diagonal\n",
    "    K_inversa = np.eye(N, k=0)*(1/m)\n",
    "    # Construimos A para el m indicado\n",
    "    A_m = construye_adyacencia(D, m)\n",
    "    A_transpueta = A_m.T\n",
    "    \n",
    "    C_m = A_transpueta @ K_inversa\n",
    "    # Devolvemos la definicion de M \n",
    "    M = (N/a)*(I-(1-a)*C_m)\n",
    "    return M\n",
    "\n",
    "def ploteador(p, m, ax =None):\n",
    "    A_m = construye_adyacencia(D, m)\n",
    "    G_m = nx.from_numpy_array(A_m) # Construimos la red a partir de la matriz de adyacencia\n",
    "    G_m_layout = {i:v for i,v in enumerate(zip(museos.to_crs(\"EPSG:22184\").get_coordinates()['x'],museos.to_crs(\"EPSG:22184\").get_coordinates()['y']))}\n",
    "\n",
    "    # Construimos un layout a partir de las coordenadas geográficas\n",
    "    factor_escala = 1e4 # Escalamos los nodos 10 mil veces para que sean bien visibles\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(15, 15)) # Visualización de la red en el mapa\n",
    "    barrios.to_crs(\"EPSG:22184\").boundary.plot(color='gray',ax=ax) # Graficamos Los barrios\n",
    "    p = p/p.sum() # Normalizamos para que sume 1\n",
    "   \n",
    "    Nprincipales = 3 # Cantidad de principales\n",
    "    principales = np.argsort(p)[-Nprincipales:] # Identificamos a los N principales\n",
    "    labels = {n: str(n) if i in principales else \"\" for i, n in enumerate(G_m.nodes)} # Nombres para esos nodos\n",
    "   \n",
    "    nx.draw_networkx(G_m, G_m_layout, node_size = p*factor_escala, ax=ax, with_labels=False) # Graficamos red\n",
    "    nx.draw_networkx_labels(G_m, G_m_layout, labels=labels, font_size=6, font_color=\"k\") # Agregamos los nombres\n",
    "\n",
    "def plot_page_rank_usando_m_o_alpha(set_museos, vectores_ranking, label_x, title, arguments_for_plot):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    for museo in list(set_museos):\n",
    "        valores_p = [p[museo] for p in vectores_ranking]\n",
    "        plt.plot(arguments_for_plot, valores_p, marker='o', label=f'Museo {museo}')\n",
    "\n",
    "    plt.xlabel(label_x)\n",
    "    plt.ylabel(\"PageRank\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aca seteamos los alpha's y m's para generar todas las matrices M que nos piden, luego vamos a obtener la \n",
    "# factorizacion LU de cada una y resolvemos con las funciones para las matrices trianguladas\n",
    "# Tambien seteamos el vector b lleno de 1's\n",
    "\n",
    "alpha_6_7 = 6/7\n",
    "alpha_4_5 = 4/5\n",
    "alpha_2_3 = 2/3\n",
    "alpha_1_2 = 1/2\n",
    "alpha_1_3 = 1/3\n",
    "alpha_1_5 = 1/5\n",
    "alpha_1_7 = 1/7\n",
    "\n",
    "m_1 = 1\n",
    "m_3 = 3\n",
    "m_5 = 5\n",
    "m_10 = 10\n",
    "\n",
    "b = np.ones(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo, antes de comenzar queremos aclarar demostrando que es lo que estamos implementando en codigo, o para qué lo estamos realizando. \n",
    "El paso a paso es el siguiente:\n",
    "$$\n",
    "M = \\frac{N}{\\alpha} \\cdot (I - (1 - \\alpha) \\cdot C)\n",
    "$$\n",
    "Para eso vamos a tener que averiguar el valor de $C$, que es:\n",
    "$$\n",
    "C = A^{t} \\cdot K^{-1}\n",
    "$$\n",
    "donde $A$ es el grafo resultado de la matriz de adyacencias $D$ y los $m$ vecinos mas cercanos, mientras que $K$ es la matriz identidad multiplicada por los $m$ vecinos.\n",
    "Una vez calculado $M$ vamos a tener que resolver:\n",
    "$$\n",
    "M^{-1}b = p\n",
    "$$\n",
    "En vez de calcular la inversa vamos a usar la factorizacion $LU$ de $M$ y desarollar el sistema a partir de ahi, por lo que el paso a paso para despejar $p$ nos va a quedar siendo:\n",
    "$$\n",
    "M = L \\cdot U\n",
    "$$\n",
    "Por lo que:\n",
    "$$\n",
    "M^{-1}b = p\n",
    "$$\n",
    "$$\n",
    "M \\cdot p = b\n",
    "$$\n",
    "$$\n",
    "L\\cdot U \\cdot p = b\n",
    "$$\n",
    "$$\n",
    "L \\cdot y = b\n",
    "$$\n",
    "Resolvemos este sistema con $L$ triangulado para obtener $y$\n",
    "$$\n",
    "U \\cdot p = y\n",
    "$$\n",
    "Resolvemos el sistema con $U$ triangulado para obtener $p$, finalizando el ejercicio y obteniendo el vector de rankings para esa matriz $M$ en particular.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Construyendo la red conectando a cada museo con sus $m$ = 3 vecinos más cercanos,\n",
    "calculen el Page Rank usando $\\alpha$ = $\\frac{1}{5}$. Visualizen la red asignando un tamaño a\n",
    "cada nodo proporcional al Page Rank que le toca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora si, armamos la primer Matriz M pedida en a)\n",
    "\n",
    "# Aca calculamos el M particular para el m y alpha fijos, calculamos su factorizacion LU y resolvemos las dos\n",
    "# triangulaciones que nos quedan, finalmente lo ploteamos\n",
    "\n",
    "M = matriz_M(alpha_1_5, m_3)\n",
    "L, U = elim_gaussiana(M)\n",
    "y = y_calculator(L, b)\n",
    "p = x_calculator(U, y)\n",
    "ploteador(p, m_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que podemos observar a partir de dejar fijos los parámetros $m$ = 3 y $\\alpha = \\frac{1}{5}$ es que los museos con mayor PageRank son aquellos que se encuentran más cercanos a otros museos. Esto tiene sentido debido al hecho de que $\\alpha$ tiene un valor bajo por lo que el valor del PageRank va a estar definido en mayor medida por la cercanía del museo (reducimos la \"aleatoreidad\"). Es por eso que aquellos museos que se encuentran entre muchos otros se ven beneficiados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Construyendo la red conectando a cada museo con sus m vecinos más cercanos,\n",
    "para $m$ = 1, 3, 5, 10 y usando $\\alpha = \\frac{1}{5}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aca ploteamos con alpha fijo en 1/5 y m variando en distintos valores (crecientes)\n",
    "cant_vecinos_cercanos = [m_1, m_3, m_5, m_10]\n",
    "vectores_p_page_ranking = []\n",
    "set_museos = set() #conjunto para no tener repetidos\n",
    "data_top3 = []\n",
    "\n",
    "# Iteramos la lista de cantidad de vecinos más cercanos (crecientes) y vamos resolviendo con factorización LU.\n",
    "# Luego ordenamos vector p para obtener los 3 mayores, los cuales vamos a guardar en un conjunto (set) para que no haya\n",
    "# repetidos a la hora de plotearlos, luego armamos los títulos para nuestro método\n",
    "# (solamente seran tenidos en cuenta aquellos museos que alguna vez hayan estado en el top 3 para cualquier alpha)\n",
    "# Se analizará el valor de PageRanking y su evolucion a medida que cambia m para los museos mencionados.\n",
    "\n",
    "fig, axs = plt.subplots(1, len(cant_vecinos_cercanos), figsize=(30, 40))\n",
    "\n",
    "for i in range(len(cant_vecinos_cercanos)):\n",
    "    m_value = cant_vecinos_cercanos[i]\n",
    "    M = matriz_M(alpha_1_5, m_value)\n",
    "    L, U = elim_gaussiana(M)\n",
    "    y = y_calculator(L, b)\n",
    "    p = x_calculator(U, y)\n",
    "    \n",
    "    ploteador(p,m_value, axs[i])\n",
    "    axs[i].set_title(f'PageRank para α = {alpha_1_5} y m = {m_value}')\n",
    "    vectores_p_page_ranking.append(p)\n",
    "    top3 = np.argsort(p)[-3:]  \n",
    "    for l in range(3):\n",
    "        set_museos.add(top3[l])\n",
    "    data_top3.append({\n",
    "        'm': m_value,\n",
    "        'Top 1': f'Museo {top3[0]}',\n",
    "        'Top 2': f'Museo {top3[1]}',\n",
    "        'Top 3': f'Museo {top3[2]}'\n",
    "    })\n",
    "arguments_for_plot = [1,3,5,10]\n",
    "label_m = 'm [cantidad de vecinos]'\n",
    "title_m = 'Variacion PageRank de top 3 museos de cada m con alpha = 1/5'\n",
    "\n",
    "plot_page_rank_usando_m_o_alpha(set_museos, vectores_p_page_ranking, label_m, title_m, cant_vecinos_cercanos)\n",
    "\n",
    "df_top3_museos = pd.DataFrame(data_top3)\n",
    "display(df_top3_museos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que podemos observar a partir de dejar fijo el parametros $\\alpha$ = $\\frac{1}{5}$ es:\n",
    "* varía significativamente la selección de top 3 museos a medida que $m$ cambia.\n",
    "* el valor de PageRank de los museos estudiados demostró una gran diferencia para cada valor de $m$.\n",
    "\n",
    "Cuando $m$ es bajo podemos ver que los museos aledaños y que se apuntan mutuamente son los de mayor PageRank. Notemos que hay mucha diferencia entre los PageRanks de los distintos nodos viendo la disparidad de tamaño entre los mismos. Los mayores PageRanks se forman en los museos que se encuetran dentro de grupos donde los elementos se encuentran más cercanos ya que la cantidad de vecinos es baja y $\\alpha$ tiene un valor relativamente bajo, por lo que no hay tantas probabilidades de que el surfer se desvíe de los museos mas cercanos.\n",
    "\n",
    "Cuando $m$ es alto vemos que los 3 mayores PageRanks estan muy cercanos el uno del otro. Esto se debe a que se encuentran en el medio de grandes conjuntos de museos, los cuales ahora son mucho mas amplios porque pueden recibir influencia de museos más lejanos, esto da como resultado que los museos con mayor valor vayan cambiando radicalmente ya que no solamente se relacionan con los museos más cercanos sino que también con museos de grupos más dispersos en la periferia. También podemos observar que los valores de PageRank tienen una tendencia a permanecer igual cuando tienen un $\\alpha$ bajo y varía $m$, lo que cambia son los museos con mayor valor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejericio C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Para $m$ = 5, considerando los valores de $\\alpha = \\frac{6}{7}, \\frac{4}{5}, \\frac{2}{3}, \\frac{1}{2}, \\frac{1}{3}, \\frac{1}{5}, \\frac{1}{7}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aca ploteamos con m fijo en 3 y alpha variando en distintos valores (crecientes)\n",
    "alphas = [alpha_1_7, alpha_1_5, alpha_1_3,  alpha_1_2, alpha_4_5, alpha_6_7]\n",
    "vectores_p_page_ranking = []\n",
    "set_museos = set()\n",
    "data_top3 = []\n",
    "\n",
    "# Iteramos la lista de cantidad de vecinos más cercanos (crecientes) y vamos resolviendo con factorización LU.\n",
    "# Luego ordenamos vector p para obtener los 3 mayores, los cuales vamos a guardar en un conjunto (set) para que no haya\n",
    "# repetidos a la hora de plotearlos, luego armamos los títulos para nuestro método\n",
    "# (solamente seran tenidos en cuenta aquellos museos que alguna vez hayan estado en el top 3 para cualquier alpha)\n",
    "# Se analizará el valor de PageRanking y su evolucion a medida que cambia alpha para los museos mencionados.\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(14, 20), tight_layout=False)\n",
    "k = 0\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        alpha = alphas[k]\n",
    "        k += 1 \n",
    "        M = matriz_M(alpha, m_3)\n",
    "        L, U = elim_gaussiana(M)\n",
    "        y = y_calculator(L, b)\n",
    "        p = x_calculator(U, y)\n",
    "        ploteador(p, m_3, axs[i, j])\n",
    "        axs[i, j].set_title(f'PageRank para α = {round(alpha, 3)} y m = {m_3}')\n",
    "        vectores_p_page_ranking.append(p)\n",
    "        top3 = np.argsort(p)[-3:]\n",
    "        for l in range(3):\n",
    "            set_museos.add(top3[l])\n",
    "        data_top3.append({\n",
    "            'α': round(alpha, 3),\n",
    "            'Top 1': f'Museo {top3[0]}',\n",
    "            'Top 2': f'Museo {top3[1]}',\n",
    "            'Top 3': f'Museo {top3[2]}'\n",
    "        })\n",
    "\n",
    "#Aca ya obtenemos los vectores de rankings p para los respectivos valores alpha y m = 3.\n",
    "\n",
    "label_a = 'alpha'\n",
    "title_a = 'Variacion PageRank de top 3 museos de cada alpha con m = 3'\n",
    "\n",
    "plot_page_rank_usando_m_o_alpha(set_museos, vectores_p_page_ranking, label_a, title_a, alphas)\n",
    "df_top3_museos = pd.DataFrame(data_top3)\n",
    "display(df_top3_museos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que podemos observar a partir de dejar fijo el parámetro $m$ = 3 y variando el valor de $\\alpha$ es que se reducen los valores del PageRank a medida que $\\alpha$ crece y $m$ permanece en un valor bajo.\n",
    "\n",
    "A diferencia del análisis anterior donde se mantuvo fijo el valor de $\\alpha$, podemos observar que la selección de top 3 de museos varió mucho menos en este nuevo estudio.\n",
    "\n",
    "Cuando $\\alpha$ tiene un valor bajo se puede observar en los mapas que los 3 museos con mayor PageRank tienen valores más grandes (mayor tamaño) y están juntos ya que se apuntan entre sí. Esto se debe a que la probabilidad de ir a un museo que no sea alguno de los 3 mas cercanos es baja por lo que el mayor PageRank se da en zonas donde se concentra una gran cantidad de museos. Esto también genera mucha más disparidad en el PageRank resultante de los museos, tenemos maximos y minimos muy amplios (podemos verlo en el tamaño de los nodos)\n",
    "\n",
    "A medida que $\\alpha$ crece aumenta la aleatoreidad del 'surfer' y por lo tanto descentraliza los valores de PageRank de los zonas donde se concentra una gran cantidad de museos y se distribuye mucho mas uniformemente. Podemos ver en el último mapa que los 3 museos con mayor PageRank estan separados entre si y los valores generales de pageRank estan repartidos mucho mas equitativamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Supongan que cada persona realiza $r$ visitas antes de abandonar la red de museos. Si\n",
    "el número total de visitas que recibió cada museo está dado por el vector $w$, tal que\n",
    "$w_i$ tiene el número total de visitantes que se recibieron en el museo $i$, muestre que el\n",
    "vector $v$, que tiene en su componente $v_i$ el número de personas que tuvo al museo $i$\n",
    "como punto de entrada a la red, puede estimarse como:\n",
    ">$$\n",
    "v = B^{-1} \\cdot w\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$w$ = Vector que representa el número total de visitas que recibió cada museo en cada componente.  \n",
    "$v$ = Vector que representa el número de visitas que recibió cada museo en el punto de partida.\n",
    "\n",
    "Al realizar producto matricial con la matriz $C$ y el vector $v$, obtengo un vector $v_1$.  \n",
    "Si se hace la misma aplicación sobre $v_1$ obtengo un $v_2$ con la información de los visitantes en una segunda iteración.  \n",
    "Este procedimiento se puede repetir $k$ veces de modo que:\n",
    "\n",
    "$$\n",
    "v_k = C^k \\cdot v\n",
    "$$\n",
    "\n",
    "(recordemos la propiedad de asociatividad $C(C \\cdot v) = (C \\cdot C) \\cdot v$\n",
    "\n",
    "$w$, al representar el número total de visitas en $r$ iteraciones, se puede describir como la sumatoria de todos los $v$ (desde $0$ a $r-1$).  \n",
    "Luego:\n",
    "\n",
    "$$\n",
    "w = \\sum_{k=0}^{r-1} v_k\n",
    "$$\n",
    "\n",
    "Sea:\n",
    "\n",
    "$$\n",
    "B = \\sum_{k=0}^{r-1} C^k\n",
    "$$\n",
    "\n",
    "Demostremos que:\n",
    "\n",
    "$$\n",
    "v = B^{-1} \\cdot w\n",
    "$$\n",
    "\n",
    "$$\n",
    "v = B^{-1} \\cdot w \\Leftrightarrow B \\cdot v = w \\Leftrightarrow \\left( \\sum_{k=0}^{r-1} C^k \\right) \\cdot v = \\sum_{k=0}^{r-1} v_k\n",
    "$$\n",
    "\n",
    "pero $v_k = C^k \\cdot v$, entonces:\n",
    "\n",
    "$$\n",
    "\\sum_{k=0}^{r-1} C^k \\cdot v = C^0 \\cdot v + C^1 \\cdot v + C^2 \\cdot v + \\dots + C^{r-1} \\cdot v = (C^0 + C^1 + C^2 + \\dots + C^{r-1}) \\cdot v = \\left( \\sum_{k=0}^{r-1} C^k \\right) \\cdot v\n",
    "$$\n",
    "\n",
    "\n",
    "Por lo tanto:\n",
    "\n",
    "$$\n",
    "v = B^{-1} \\cdot w \\quad \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aca vamos a describir que pasos vamos a llevar a cabo en cada item del ejercicio, comenzando con el item a basicamente calculamos:\n",
    "$$\n",
    "C_{ji} = P(i \\rightarrow j) = \\frac{f(d_{ij})}{\\sum_{\\substack{k=1, k\\ne i}}^{N} f(d_{ik})}\n",
    "$$\n",
    "Luego para item b, implementamos la siguiente formula para calcular B ahora que tenemos C a partir del item anterior\n",
    "$$\n",
    "B = \\sum_{k=0}^{r-1} C^k\n",
    "$$\n",
    "y finalmente para el item c, factorizamos B para hallar:\n",
    "$$\n",
    "v = B^{-1} \\cdot w \\quad \n",
    "$$\n",
    "Por lo que el procedimiento implementado para resolverlo fue el siguiente:\n",
    "$$\n",
    "B \\cdot v = w\n",
    "$$\n",
    "$$\n",
    "LU \\cdot v = w\n",
    "$$\n",
    "$$\n",
    "L \\cdot y = w\n",
    "$$\n",
    "Resolvemos con el L triangulado para obtener w, y resolvemos:\n",
    "$$\n",
    "U \\cdot v = y\n",
    "$$\n",
    "Resolvemos el sistema con U triangulado y finalmente obtenemos v, al cual si le calculamos la norma obtenemos lo que estabamos buscando, que era la cantidad de visitantes que entraron en la red\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seteamos variables globales que vamos a necesitar\n",
    "\n",
    "vector_w = pd.read_csv('./visitas.txt', sep='\\t', header=None).squeeze(\"columns\").values\n",
    "r=3\n",
    "\n",
    "\n",
    "#Ejercicio a)\n",
    "\n",
    "# En Calcula_matriz_C_continua(D) lo que hacemos es recorrer D (matriz de distancias de museos) y calculamos F que es la matriz 1/D, luego \n",
    "# vamos calculando la sumatoria de la fila de F hasta que finalizemos de recorrerla y una vez que finalizamos calculamos k inversa, que al ser\n",
    "# k la matriz con unicamente la sumatoria de f en su diagonal entonces su inversa la podemos calcular como 1/sumatoria_fila, finalmente\n",
    "# llegamos a C haciendo la muplicacion matricial K_inv @ F\n",
    "\n",
    "def calcula_matriz_C_continua(D):\n",
    "    N = D.shape[0]\n",
    "    sumatoria_fila = 0\n",
    "    F = np.zeros((N,N))\n",
    "    K_inv = np.zeros((N,N))\n",
    "    \n",
    "    for fila in range(N):\n",
    "        sumatoria_fila = 0\n",
    "        for columna in range(N):\n",
    "            if fila!=columna:\n",
    "                F[fila][columna] = 1/D[fila][columna]\n",
    "                sumatoria_fila += F[fila][columna]\n",
    "        for columna in range(N):\n",
    "            if fila==columna:\n",
    "                K_inv[fila][columna] = 1/sumatoria_fila\n",
    "    return F @ K_inv\n",
    "\n",
    "# Ejercicio b)\n",
    "\n",
    "# Aca calculamos la matriz B como la sumatoria de las potencias de C^k con k yendo de 0 hasta r-1, \n",
    "\n",
    "def calcula_B(C, cantidad_de_visitas):\n",
    "    B = np.eye(C.shape[0])  # C^0\n",
    "    C_k = np.eye(C.shape[0])\n",
    "    for k in range(cantidad_de_visitas-1):\n",
    "        C_k = C_k @ C\n",
    "        B += C_k\n",
    "    return B\n",
    "\n",
    "# Ejercicio c)\n",
    "\n",
    "# Aca finalmente resolvemos directamente como vinimos haciendo anteriormente en el tp, con factorizacion LU en vez de invertir B, con su debida \n",
    "# justificacion arriba, las funciones son las definidas previamente tambien mas arriba en el trabajo. Luego resolvemos y obtenemos calculando\n",
    "# norma 1 de el vector solucion v para obtener la cantidad de visitas\n",
    "\n",
    "C = calcula_matriz_C_continua(D)\n",
    "B = calcula_B(C,r)\n",
    "L, U = elim_gaussiana(B)\n",
    "y = y_calculator(L, vector_w)\n",
    "v = x_calculator(U, y)\n",
    "\n",
    "cantidad_visitas = np.linalg.norm(v, 1)\n",
    "print(cantidad_visitas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $w$ un vector que representa las visitas totales de cada museo, estimamos $v$ (visitas inciales en cada museo).\n",
    "\n",
    "Dado que $w$ podría tener un error de hasta el 5%, queremos estimar el error que se podría obtener al realizar la transformación B sobre el vector $w$, tal que\n",
    "\n",
    "$$\n",
    "v = B^{-1} \\cdot w\n",
    "$$\n",
    "\n",
    "Para ello, queremos calcular el número de condición de $B$ para poder estimar una cota superior sobre el error de $v$ tal que\n",
    "\n",
    "$$\n",
    "\\frac{\\|v\\|_1 - \\|\\tilde{v}\\|_1}{\\|v\\|_1} \\leq cond_1(B) \\cdot \\frac{\\|w\\|_1 - \\|\\tilde{w}\\|_1}{\\|w\\|_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norma1(M):\n",
    "    res = np.sum(np.abs(M), axis=0).max()\n",
    "    return res\n",
    "\n",
    "\n",
    "# Calculamos la inversa de B con su factorizacion LU para obtener el numero de condicion\n",
    "def inversa(L, U):\n",
    "    # Calculamos numero de la condición y con él resolvemos\n",
    "    n = L.shape[0]\n",
    "    identidad = np.eye(n)\n",
    "    \n",
    "    columna_inversa_B = []\n",
    "    for i in range(n):\n",
    "        columna_i = identidad[i]\n",
    "        y = y_calculator(L, columna_i)\n",
    "        x = x_calculator(U, y)\n",
    "        columna_inversa_B.append(x)\n",
    "    return np.column_stack(columna_inversa_B)\n",
    "\n",
    "inversa_B_2 = inversa(L, U)\n",
    "\n",
    "# Calculamos la norma de B y B^(-1) para multiplicarlas y obtener cond(B)\n",
    "norma_1_B = norma1(B)\n",
    "norma_1_inv_B = norma1(inversa_B_2)\n",
    "\n",
    "# Calculamos el numero de condicion\n",
    "cond_B = norma_1_B * norma_1_inv_B\n",
    "\n",
    "# Multiplimos el numero de condicion por 0.05 para obtener una cota superior del error de v\n",
    "error = 0.05\n",
    "error_v = cond_B * error\n",
    "print(error_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTE 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio A.\n",
    "> Muestren que el vector de unos $1_v$ es autovector de las matrices $R$ y $L$. ¿Qué\n",
    "autovalor tiene? ¿Y qué agrupacién de la red representa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$1_v$ es autovector de $L \\Rightarrow$ Existe $\\lambda \\in \\mathbb{R}$ tal que $L \\cdot 1_v = \\lambda \\cdot 1_v$.\n",
    "\n",
    "$$L = K - A \\Rightarrow (K - A) \\cdot 1_v = K \\cdot 1_v - A \\cdot 1_v$$\n",
    "\n",
    "Sea el $k$, aquel vector con $k_i$ , la cantidad de conexiones de $i$. \n",
    "Tal que $k_i = \\sum_{i=1}^{n} A_{ij}$\n",
    "\n",
    "\n",
    "$$\n",
    "K = \\begin{pmatrix}\n",
    "k_1 & 0 & \\cdots & 0 \\\\\n",
    "0 & k_2 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & k_n\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Luego, $K * 1_v = k$.\n",
    "\n",
    "Por otro lado,\n",
    "\n",
    "$$A * 1_v  =  (\\sum_{j=1}^{n} A_{1j}, \\sum_{j=1}^{n} A_{2j}, \\dots, \\sum_{j=1}^{n} A_{nj})  =  (k_1, ... ,k_n)  =  k$$\n",
    "\n",
    "Entonces, tengo que \n",
    "\n",
    "$$K * 1_v - A * 1_v = k - k = 0 = \\lambda * 1_v.$$\n",
    "\n",
    "Luego, si $\\lambda * 1_v = 0 \\Rightarrow \\lambda = 0$.\n",
    "\n",
    "\n",
    "Veamos que:\n",
    "\n",
    "$\\lambda = \\frac{1}{4} * s^t * L * s$  siendo $s$ el vector tal que $s_i = 1$ si $i$ pertenece al grupo $1$ y $s_i = -1$ si $i$ pertenece al grupo $2$.\n",
    "\n",
    "Y $\\lambda$ representa la cantidad de cortes.\n",
    "\n",
    "Tomemos a $s = 1_v$ \n",
    "\n",
    "$$\\Rightarrow \\lambda = \\frac{1}{4} * 1_v^t * L * 1_v = \\frac{1}{4} * (0 * 1_v) = \\frac{1}{4} * 0 = 0$$\n",
    "\n",
    "Entonces $\\lambda = 0$.\n",
    "\n",
    "Esto representa el caso en el que todos los elementos pertenecen al mismo grupo, por lo que no existe ningun corte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1_v$ es autovector de $L \\Rightarrow$ Existe $\\lambda \\in \\mathbb{R}$ tal que $L * 1_v = \\lambda * 1_v$.\n",
    "\n",
    "$$R = A - P \\Rightarrow (A - P) * 1_v = A * 1_v - P * 1_v.$$\n",
    "\n",
    "llamemos $P * 1_v = h$.\n",
    "\n",
    "$$h_i = \\sum_{j=1}^{n} \\frac{k_i \\cdot k_j}{2E} = \\frac{ki}{2E} * \\sum_{j=1}^{n} k_j.$$\n",
    "\n",
    "\n",
    "Pero \n",
    "\n",
    "$$2E = \\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{ij} = \\sum_{i=1}^{n} k_j.$$\n",
    "\n",
    "Luego, \n",
    "\n",
    "$$\\frac{ki}{2E} * \\sum_{j=1}^{n} k_j = \\frac{ki}{\\cancel{2E}} * \\cancel{2E} = k_i \\Rightarrow h_i = k_i \\Rightarrow h = k \\Rightarrow P * 1_v = k.$$\n",
    "\n",
    "\n",
    "Entonces, como $A * 1_v = k$  y \n",
    "\n",
    "$$P * 1_v = k \\Rightarrow  A * 1_v - P * 1_v = k - k = 0 = \\lambda * 1_v.$$\n",
    "\n",
    "Luego, si $\\lambda * 1_v = 0 \\Rightarrow \\lambda = 0$.\n",
    "\n",
    "\n",
    "Veamos que,\n",
    "\n",
    "$$Q = \\frac{1}{4E} * 1_v^t * R * 1_v = \\frac{1}{4E} * 1_v^t * 0 = 0.$$\n",
    "\n",
    "Esto representa que para $s = 1_v$ (todos los elementos del mismo grupo), la modularidad es $0$. \n",
    "\n",
    "Esto quiere decir, que hay las mismas conexiones entre ellos que las que esperamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio B.\n",
    "\n",
    "> Muestren que si L (R) tienen dos autovectores v1 y v2 asociados a autovalores\n",
    "$\\lambda_1 \\neq \\lambda_2$, entonces $v_1^t * v_2 = 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos que $L$ y $R$ son matrices simétricas.\n",
    "Tomo una matriz $M$ genérica, trato de mostrar que si $M$ es simétrica con 2 autovalores $\\lambda_1$ y $\\lambda_2$. Entonces, sus autovectores asociados ($v_1$ y $v_2$, respectivamente) son ortogonales entre sí, esto significa que $v_1^t * v_2 = 0$.\n",
    "\n",
    "Sea $v_2$ autovector de $M$ \n",
    "\n",
    "$$\\Rightarrow M * v_2 = \\lambda_2 * v_2$$ \n",
    "\n",
    "$$\\Leftrightarrow v_1^t * (M * v_2) = v_1^t * (\\lambda_2 * v_2)$$ \n",
    "\n",
    "$$\\Leftrightarrow v_1^t * M * v_2 = \\lambda_2 * v_1^t v_2$$\n",
    "\n",
    "Sea $v_1$ autovector de $M$ \n",
    "\n",
    "$$\\Rightarrow M * v_1 = \\lambda_1 * v_1 \\Leftrightarrow v_2^t * (M * v_1) = v_2^t * (\\lambda_1 * v_1)$$\n",
    "\n",
    "$$\\Leftrightarrow (v_2^t * M * v_1)^t = (v_2^t * \\lambda_1 * v_1)^t$$\n",
    "\n",
    "$$\\Leftrightarrow v_1^t * M^t * (v_2^t)^t = v_1^t * \\lambda_1 * (v_2^t)^t$$\n",
    "\n",
    "$$\\Leftrightarrow v_1^t * M^t * v_2 = v_1^t * \\lambda_1 * v_2$$\n",
    "\n",
    "pero $M^t = M$, entonces:\n",
    "\n",
    "$$v_1^t * M^t * v_2 = v_1^t * \\lambda_1 * v_2$$\n",
    "\n",
    "$$\\Leftrightarrow v_1^t * M * v_2 = \\lambda_1 * v_1^t * v_2$$\n",
    "\n",
    "En resumen:\n",
    "\n",
    "$v_2$ autovector de $M \\Rightarrow v_1^t * M * v_2 = \\lambda_2 * v_1^t * v_2$\n",
    "\n",
    "$v_1$ autovector de $M \\Rightarrow v_1^t * M * v_2 = \\lambda_1 * v_1^t * v_2$\n",
    "\n",
    "Igualando ambas ecuaciones:\n",
    "$\\lambda_2 * (v_1^t * v_2) = \\lambda_1 * (v_1^t * v_2)$\n",
    "\n",
    "Esto es posible si y solo si:\n",
    "\n",
    "$\\lambda_1 = \\lambda_2$ ó $(v_1^t * v_2) = 0.$\n",
    "\n",
    "\n",
    "Entonces, queda demostrado que, si $\\lambda_1  \\neq  \\lambda_2 \\Rightarrow v_1^t * v_2 = 0$. Para toda matríz M simétrica.\n",
    "\n",
    "Como $R$ y $L$ son matrices simétricas, queda mostrado que dos autovectores $v_1$ y $v_2$ asociados a autovalores $\\lambda_1 \\neq \\lambda_2$, entonces $v_1^t * v_2 = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio C.\n",
    " >Muestren si v es un autovector de autovalor λ ̸= 0 de R o L, entonces sumatoria desde i = 0 hasta n de v_i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos que:\n",
    "\n",
    "* por punto a: $1_v$ es autovector de las matrices $R$ y $L$ con autovalor $0$ para ambos.\n",
    "\n",
    "* por punto b: Tanto para $L$ como para $R$, se cumple que, para autovalores $\\lambda_1 \\neq \\lambda_2 \\Rightarrow$ autovectores asociados $v_1$, $v_2$: $v_1^t * v_2 = 0$.\n",
    "\n",
    "Tomando $v_1 = 1_v$ y $v_2$ autovector con autovalor $\\neq 0$:\n",
    "\n",
    "$$v_1^t * v_2 = 0 \\Leftrightarrow 1_v^t * v_2 = 0$$ \n",
    "\n",
    "$$\\Leftrightarrow 1 * (v_2)_1 + 1 * (v_2)_2 + ... + 1 * (v_2)_n = 0$$\n",
    "\n",
    "$$\\Leftrightarrow \\sum_{i=1}^{n} (v_2)_i = 0.$$\n",
    "\n",
    "Luego, queda demostrado que para cualquier autovalor distinto de $0$, la suma de los elementos de su autovector asociado equivale a $0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio A.\n",
    "> Shifting de autovalores: Muestre que los autovalores de $M + \\mu I$ son $\\gamma_i = \\lambda_i + \\mu$, y que el autovector asociado a $\\gamma_i$ es $v_i$.\n",
    "Concluya que si $\\mu + \\lambda_i \\ne 0 \\ \\forall i$, entonces $M + \\mu I$ es inversible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para demostrar esto vamos a comenzar viendo que:\n",
    "$$\n",
    "(M + \\mu I) \\cdot V_i = (\\lambda_i + \\mu) \\cdot V_i \\equiv  \n",
    "$$\n",
    "$$\n",
    "(M + \\mu I) \\cdot V_i - (\\lambda_i + \\mu) \\cdot V_i = 0 \\equiv\n",
    "$$\n",
    "$$\n",
    "(M + \\mu I - \\lambda_i I - \\mu I) \\cdot V_i = 0 \\equiv  \n",
    "$$\n",
    "$$\n",
    "(M - \\lambda_i I )\\cdot V_i = 0 \\equiv  \n",
    "$$\n",
    "$$\n",
    "M \\cdot V_i = \\lambda_i \\cdot V_i  \n",
    "$$\n",
    "Lo que podemos observar es que si $\\lambda_i$ es autovalor de $M$, entonces $(\\lambda_i + \\mu)$ es autovalor de $(M + \\mu I)$, tambien podemos ver que efectivamente $V_i$ no cambia por lo que el autovector asociado al autovalor no cambia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio B.\n",
    "\n",
    "> Método de la potencia inverso: Considerando $\\mu$ > 0, muestren que $L + \\mu I$ es\n",
    "inversible, con L el laplaciano definido en la ecuación 2. Muestren que aplicar el\n",
    "método de la potencia a $(L + \\mu I)^{-1}$\n",
    "converge a su autovector de autovalor más\n",
    "chico si se parte de una semilla adecuada. Indique, en el caso de que hay sólo un\n",
    "autovector con dicho autovalor, cuál es dicho autovector y cuánto vale su autovalor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como sabemos por el punto 1, $\\lambda = 0$ es un autovalor de la matriz Laplaciana $L$, y su autovector asociado es el vector constante de unos $$\\mathbf{1} = (1, 1, \\ldots, 1)$$ \n",
    "Ademas se nos dice que $L$ es una matriz simétrica y semidefinida positiva, esto quiere decir que todos sus autovalores $\\lambda_i$ son Reales y $\\geq 0$.\n",
    "\n",
    "Al considerar la matriz $(L + \\mu I)$, con $(\\mu > 0)$, los nuevos autovalores son:\n",
    "$$\n",
    "\\lambda_i + \\mu > 0 \\quad \\forall i\n",
    "$$\n",
    "Esto vale por lo demostrado en el item a). Por lo que todos los autovalores de la matriz son diferentes de 0 y por lo tanto es inversible.\n",
    "\n",
    "Ahora consideramos la matriz inversa $(L + \\mu I)^{-1}$. Sabemos que  $L$ y $L + \\mu I$ comparten los mismos autovectores, veamos que comparten los mismos autovectores con su inversa:\n",
    "$$\n",
    "L \\cdot v_i = \\lambda_i \\cdot v_i \\equiv\n",
    "$$\n",
    "$$\n",
    "(L + \\mu I) \\cdot v_i = (\\lambda_i + \\mu) \\cdot v_i\n",
    "$$\n",
    "\n",
    "Ahora, como $(L + \\mu I)$ es inversible por lo demostrado anteriormente podemos aplicar la inversa de ambos lados:\n",
    "\n",
    "$$\n",
    "(L + \\mu I)^{-1} (L + \\mu I) \\cdot v_i = (L + \\mu I)^{-1} (\\lambda_i + \\mu) \\cdot v_i \\equiv\n",
    "$$\n",
    "$$\n",
    "I \\cdot v_i = (L + \\mu I)^{-1} (\\lambda_i + \\mu) \\cdot v_i \\equiv\n",
    "$$\n",
    "$$\n",
    "v_i = (\\lambda_i + \\mu)(L + \\mu I)^{-1} \\cdot v_i \\equiv\n",
    "$$\n",
    "$$\n",
    "\\frac{1}{\\lambda_i + \\mu} \\cdot v_i = (L + \\mu I)^{-1} \\cdot v_i \\equiv\n",
    "$$\n",
    "$$\n",
    "(L + \\mu I)^{-1} \\cdot v_i = \\frac{1}{\\lambda_i + \\mu} \\cdot v_i \n",
    "$$\n",
    "Por lo que $v_i$ es el mismo autovector para $(L + \\mu I)^{-1}$ y su autovalor asociado resulta siendo: \n",
    "$\\frac{1}{\\lambda_i + \\mu}$ $\\forall \\lambda_i$ autovalor de $L$\n",
    "\n",
    "Como $\\lambda_i + \\mu > 0$, todos los autovalores $\\frac{1}{\\lambda_i + \\mu}$ son positivos y están bien definidos.\n",
    "\n",
    "Ahora bien, como $L$ es semidefinida positiva entonces todos sus autovalores son $\\geq 0$, y sabemos que 0 es autovalor de L, por lo que podemos asegurar que se cumple lo siguiente para la matriz $(L + \\mu I)^{-1}$:\n",
    "$$\n",
    "\\max_i \\left\\{ \\frac{1}{\\lambda_i + \\mu} \\right\\} = \\frac{1}{\\mu}\n",
    "$$\n",
    "\n",
    "A partir de esto sabemos que si aplicamos el metodo de la potencia, el cual dice que si A es diagonalizable y su autovalor de mayor módulo es único, entonces este converge al autovector asociado a ese autovalor a partir de un a semilla inicial adecuada, por lo que solamente nos falta demostrar que $(L + \\mu I)^{-1}$ es diagonalizable:\n",
    "\n",
    "$L$ es simétrica y por lo tanto diagonalizable en una base ortonormal, esto se puede desprender a partir del teorema espectral, por lo que esto significa que existe matriz ortogonal $Q$ y una matriz diagonal $D$ tal que\n",
    "$$\n",
    "L =\\ QDQ^T,\n",
    "$$\n",
    "donde\n",
    "$$\n",
    "D =\\ \\mathrm{diag}(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)\n",
    "$$\n",
    "Siendo cada $\\lambda_i$ un autovalor $L$.\n",
    "\n",
    "$$\n",
    "(L + \\mu I) =\\ QDQ^T + \\mu I.\n",
    "$$\n",
    "$$\n",
    "(L + \\mu I) = QDQ^T + Q\\mu IQ^T \\equiv Q(D + \\mu I)Q^T.\n",
    "$$\n",
    "\n",
    "Como $D$ es diagonal y $\\mu I$ tambien, entonces la suma de estos es diagonal y va a contener los autovalores de forma $(\\lambda + \\mu)$ que efectivamente son los autovalores de $(L + \\mu I)$. \n",
    "\n",
    "Por lo visto anteriormente sabemos que existe la inversa de esta matriz por lo que D va a estar compuesta de los autovalores $\\frac{i}{\\lambda_i + \\mu}$ en su diagonal. \n",
    "Ahora, por propiedad sabemos que $(QDQ^T)^{-1} = QD^{-1}Q^T$ (dado que $Q^T = Q^{-1}$), por lo que finalmente obtenemos:\n",
    "$$\n",
    "(L + \\mu I)^{-1} = Q(D + \\mu I)^{-1}Q^T\n",
    "$$\n",
    "Llegamos a que $(L + \\mu I)^{-1}$ es diagonalizable con autovalor mayor de valor unico, entonces a partir de un $x_0$ adecuado vamos a obtener el autovector asociado al mayor autovalor de la matriz. Por definicion un $x_0$ adecuado seria aquel que tenga un componente no nulo en la dirección del autovector dominante, por lo que en nuestro caso donde ese autovector es el $\\mathbf{1} = (1, 1, \\ldots, 1)$ va a ser aquel tal que $x_0^{t} \\cdot \\mathbf{1} \\neq 0$. \n",
    "\n",
    "El autovalor obtenido va a ser el autovalor mas chico de la matriz $(L + \\mu I)$ (esto se debe a la relacion entre los autovalores de una matriz y su inversa explicitada anteriormente), entonces es decir, convergera a $$\\mathbf{1} = (1, 1, \\ldots, 1)$$ con autovalor $\\frac{1}{\\mu}$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio C.\n",
    "\n",
    "> Deflación de Hotelling: Suponiendo que $M$ es simétrica (y por lo tanto admite una base ortogonal de autovectores), muestre que la matriz\n",
    ">$$\n",
    ">\\tilde{M} = M - \\frac{\\lambda_1 v_1 v_1^t}{v_1^t v_1}\n",
    ">$$\n",
    "> tiene los mismos autovectores que $M$, pero el autovalor asociado a $v_1$ es igual a cero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para demostrar esto vamos a comenzar con que como M es simetrica entonces admite una base ortogonal de autovectores, este dato nos va a servir para luego, ahora demostremos que para todo $v_k$ con $k \\neq 1$  vale que la matriz $M - \\lambda_1 \\cdot \\frac{v_1 \\cdot v_1^{t}}{v_1^{t} \\cdot v_1}$ tiene los mismos autovectores que la matriz $M$:\n",
    "$$\n",
    "(M - \\lambda_1 \\cdot \\frac{v_1 \\cdot v_1^{t}}{v_1^{t} \\cdot v_1}) \\cdot v_k = (\\lambda_k - \\lambda_1 \\cdot \\frac{v_1 \\cdot v_1^{t}}{v_1^{t} \\cdot v_1}) \\cdot v_k\n",
    "$$\n",
    "$$\n",
    "(M \\cdot v_k - \\lambda_1 \\cdot \\frac{v_1 \\cdot v_1^{t} \\cdot v_k}{v_1^{t} \\cdot v_1}) = (\\lambda_k \\cdot v_k - \\lambda_1 \\cdot \\frac{v_1 \\cdot v_1^{t} \\cdot v_k}{v_1^{t} \\cdot v_1}) \n",
    "$$\n",
    "Ahora, como tenemos que M es simetrica y que por lo tanto admite una base ortogonal de autovectores, esto significa que para todo $v_i, v_j$ con $i \\neq j$ y $\\lambda_i \\neq \\lambda_j$, que $v_i \\cdot v_j$ = 0 (son ortogonales entre si)\n",
    "Entonces como por hipotesis $v_k$ es autovector de de M y $k \\neq 1$ entonces $v_1^{t} \\cdot v_k = 0$\n",
    "Por lo que queda\n",
    "$$\n",
    "(M \\cdot v_k - \\lambda_1 \\cdot \\frac{v_1 \\cdot 0}{v_1^{t} \\cdot v_1}) = (\\lambda_k \\cdot v_k - \\lambda_1 \\cdot \\frac{v_1 \\cdot 0}{v_1^{t} \\cdot v_1})\n",
    "$$\n",
    "$$\n",
    "(M \\cdot v_k - \\lambda_1 \\cdot 0 )= (\\lambda_k \\cdot v_k - \\lambda_1 \\cdot 0)\n",
    "$$\n",
    "$$\n",
    "M \\cdot v_k = \\lambda_k \\cdot v_k\n",
    "$$\n",
    "Y esta es la definicion de autovalores y autovectores de M, por lo que $\\forall v_k / k \\neq 1$ se tiene que son los mismo autovectores que M. \n",
    "Ahora, veamos porque esto no vale para $v_1$, lo que vamos a hacer para demostrarlo es que para $v_1$ efectivamente nos queda igualado a 0:\n",
    "\n",
    "$$\n",
    "(M - \\lambda_1 \\cdot \\frac{v_1 \\cdot v_1^t}{v_1^{t} \\cdot v_1}) \\cdot v_1 = 0\n",
    "$$\n",
    "$$\n",
    "M \\cdot v_1 - \\lambda_1 \\cdot \\frac{v_1 \\cdot v_1^t}{v_1^{t} \\cdot v_1} = 0\n",
    "$$\n",
    "$$\n",
    "\\lambda_1 \\cdot v_1 - \\lambda_1 \\cdot \\frac{v_1 \\cdot v_1^{t} \\cdot v_1}{v_1^{t} \\cdot v_1} = 0\n",
    "$$\n",
    "Nos queda $v_1^t \\cdot v_1$ en ambas partes de la fraccion, esto es equivalente $\\|v\\|^2$, ya que estamos multiplicando cada coordenada de v por si misma. por lo que obtenemos:\n",
    "$$\n",
    "\\lambda_1 \\cdot v_1 - \\lambda_1 \\cdot \\frac{v_1 \\cdot \\|v\\|^2}{\\|v\\|^2} = 0\n",
    "$$\n",
    "$$\n",
    "\\lambda_1 \\cdot v_1 - \\lambda_1 \\cdot v_1 = 0\n",
    "$$\n",
    "Lo cual es verdadero y por lo tanto llegamos a que $v_1$ es autovector de la matriz pero su autovalor asociado es el 0 para $M - \\lambda_1 \\cdot \\frac{v_1 \\cdot v_1^{t}}{v_1^{t} \\cdot v_1}$. Quedando demostrado que esta matriz comparte los mismos autovectores que $M$ pero el autovalor asociado a $v_1$ es 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### *i*.\n",
    ">*calcula_L($A$)* y *calcula_R($A$)* que reciban la matriz de adyacencia $A$ y retornen las matrices $L$ y $R$ respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helpers\n",
    "\n",
    "def vector_1(n):\n",
    "  return np.zeros((n, 1)) + 1\n",
    "\n",
    "\n",
    "def calcula_K(A):\n",
    "  n = A.shape[0]\n",
    "  K = np.zeros((n, n))\n",
    "  for i in range(n):\n",
    "    K[i][i] = np.sum(A[i])\n",
    "  return K\n",
    "\n",
    "def calcula_vector_k(A):\n",
    "  return calcula_K(A) @ vector_1(A.shape[0]);\n",
    "\n",
    "def cant_conexiones(A):\n",
    "  return A.sum() / 2\n",
    "\n",
    "def calcula_P(A):\n",
    "  k = calcula_vector_k(A)\n",
    "  P = k * k.T / (2*cant_conexiones(A))\n",
    "  return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_L(A):\n",
    "  return calcula_K(A) - A\n",
    "\n",
    "\n",
    "def calcula_R(A):\n",
    "    return A - calcula_P(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### *ii.*\n",
    "> *calcula_lambda($L$,$v$)* y *calcula_Q($R$,$v$)* que reciban las matrices $L$ y $R$, y\n",
    "un autovector $v$, y retornen el corte $Λ$ y la modularidad $Q$ asociados a $v$.\n",
    "Calculen con ellas el corte y la modularidad de las particiones esperadas en la\n",
    "red de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_lambda(L,v):\n",
    "    s = np.sign(v)\n",
    "    return s.T/4 @ L @ s\n",
    "\n",
    "def calcula_Q(R,v):\n",
    "    s = np.sign(v)\n",
    "    return s.T @ R @ s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### *i.*\n",
    "> Una función *metpot1($M$)* que reciba una matriz $M$ y use el método de la\n",
    "potencia para retornar el autovalor de mayor módulo y su correspondiente\n",
    "autovector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def metpot1(A, tol=1e-8, maxrep=np.inf):\n",
    "    v = np.random.rand(A.shape[0])\n",
    "    v = v / np.linalg.norm(v, 2) #normalizo\n",
    "    v1 = A @ v\n",
    "    v1 = v1 / np.linalg.norm(v1, 2)\n",
    "    l = (v.T @ A @ v) / (v.T @ v)\n",
    "    l1 = (v1.T @ A @ v1) / (v1.T @ v1)\n",
    "    nrep = 0\n",
    "    while  abs(np.linalg.norm(v1 - v, 2)) > tol and nrep < maxrep:\n",
    "        v = v1\n",
    "        l = l1\n",
    "        v1 = A @ v\n",
    "        v1 = v1 / np.linalg.norm(v1, 2)\n",
    "        l1 = (v1.T @ A @ v1) / (v1.T @ v1)\n",
    "        nrep += 1\n",
    "    l = (v1.T @ A @ v1) / (v1.T @ v1)\n",
    "    return v1, l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### *ii.*\n",
    ">Una función *deflaciona($M$)* que reciba una matriz $M$, calcule su primer autovector y autovalor, y calcule su versión deflacionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deflaciona(A,tol=1e-8,maxrep=np.inf):\n",
    "    # Recibe la matriz A, una tolerancia para el método de la potencia, y un número máximo de repeticiones\n",
    "    v1,l1 = metpot1(A,tol,maxrep) # Buscamos primer autovector con método de la potencia\n",
    "    deflA = A - l1 * np.outer(v1, v1) # Sugerencia, usar la funcion outer de numpy\n",
    "    return deflA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### *iii.*\n",
    "> Una función *metpotI($M$,$\\mu$)* que reciba una matriz $M$ y un coeficiente $\\mu$ y calcule el autovalor más chico de $M + \\mu$ junto a su autovector asociado,\n",
    "usando el método de la potencia inversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def metpot_inv(A, tol=1e-8, maxrep=np.inf):\n",
    "  L, U = elim_gaussiana(A)\n",
    "  A_inv = inversa(L, U)\n",
    "  vect, val = metpot1(A_inv, tol=tol, maxrep=maxrep)\n",
    "  return vect, 1/val\n",
    "\n",
    "def metpotI(A, mu, tol=1e-8, maxrep=np.inf):\n",
    "    # Retorna el primer autovalor de la inversa de A + mu * I, junto a su autovector y si el método convergió.\n",
    "    shift = A + mu * np.eye(A.shape[0])\n",
    "    return metpot_inv(shift, tol=tol, maxrep=maxrep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### *iv.*\n",
    "> Una función *metpotI2($M$,$\\mu$)* que reciba una matriz $M$ y un coeficiente $\\mu$ y calcule el segundo autovalor más chico, con su autovector asociado, de la\n",
    "matriz $M + \\mu I$, y bajo la suposición de que todos los autovectores de M son\n",
    "no-negativos, y sólo uno de ellos es igual a cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metpotI2(A, mu=0, tol=1e-8, maxrep=np.inf):\n",
    "    L, U = elim_gaussiana(A)\n",
    "    A_inv = inversa(L, U)\n",
    "    A_def = deflaciona(A_inv,tol=1e-8,maxrep=np.inf)\n",
    "    vect, val = metpot1(A_def, tol=tol, maxrep=maxrep)\n",
    "    return vect, 1/val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### *i.*\n",
    ">*laplaciano_iterativo($A$,niveles)* que reciba la matriz de adyacencia $A$ y\n",
    "el número de niveles que se debe alcanzar realizando particiones iterativamente (para *niveles* = $k$ se obtienen $2^k$ particiones). La función debe calcular el\n",
    "laplaciano $L$, y recursivamente partir la red hasta llegar a n niveles de partición. El resultado debe ser una lista de listas, donde cada sub-lista contiene\n",
    "los indices de los nodos correspondientes a una misma comunidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplaciano_iterativo(A,niveles,nombres_s=None):\n",
    "    # Recibe una matriz A, una cantidad de niveles sobre los que hacer cortes, y los nombres de los nodos\n",
    "    # Retorna una lista con conjuntos de nodos representando las comunidades.\n",
    "    # La función debe, recursivamente, ir realizando cortes y reduciendo en 1 el número de niveles hasta llegar a 0 y retornar.\n",
    "    if nombres_s is None: # Si no se proveyeron nombres, los asignamos poniendo del 0 al N-1\n",
    "        nombres_s = range(A.shape[0])\n",
    "    if A.shape[0] == 1 or niveles == 0: # Si llegamos al último paso, retornamos los nombres en una lista\n",
    "        return([nombres_s])\n",
    "    else: # Sino:\n",
    "        mu_identidad = np.eye(A.shape[0])*2\n",
    "        L = calcula_L(A) + mu_identidad # Recalculamos el L\n",
    "\n",
    "        v,l =  metpotI2(L)# Encontramos el segundo autovector de L\n",
    "        # Recortamos A en dos partes, la que está asociada a el signo positivo de v y la que está asociada al negativo\n",
    "        Ap = A[v >= 0][:, v >= 0] # Asociado al signo positivo\n",
    "        Am = A[v < 0][:, v < 0] # Asociado al signo negativo\n",
    "        return(\n",
    "                laplaciano_iterativo(Ap,niveles-1,\n",
    "                                     nombres_s=[ni for ni,vi in zip(nombres_s,v) if vi>0]) +\n",
    "                laplaciano_iterativo(Am,niveles-1,\n",
    "                                     nombres_s=[ni for ni,vi in zip(nombres_s,v) if vi<0])\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### *ii.*\n",
    "> *modularidad_iterativo($A$)* que reciba la matriz de adyacencia $A$ y compute\n",
    "la matriz de modularidad $R$. Luego debe realizar iterativamente particiones en\n",
    "las comunidades del grafo en mitades. El algoritmo debe detenerse cuando las\n",
    "siguientes divisiones no aumentan la modularidad total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modularidad_iterativo(A=None,R=None,nombres_s=None):\n",
    "    # Recibe una matriz A, una matriz R de modularidad, y los nombres de los nodos\n",
    "    # Retorna una lista con conjuntos de nodos representando las comunidades.\n",
    "\n",
    "    if A is None and R is None:\n",
    "        print('Dame una matriz')\n",
    "        return(np.nan)\n",
    "    if R is None:\n",
    "        R = calcula_R(A)\n",
    "    if nombres_s is None:\n",
    "        nombres_s = range(R.shape[0])\n",
    "    # Acá empieza lo bueno\n",
    "    if R.shape[0] == 1: # Si llegamos al último nivel\n",
    "         return([nombres_s])\n",
    "    else:\n",
    "        v,l = metpot1(R) # Primer autovector y autovalor de R\n",
    "        # Modularidad Actual:\n",
    "        Q0 = np.sum(R[v>0,:][:,v>0]) + np.sum(R[v<0,:][:,v<0])\n",
    "        if Q0<=0 or all(v>0) or all(v<0): # Si la modularidad actual es menor a cero, o no se propone una partición, terminamos\n",
    "            return([nombres_s])\n",
    "        else:\n",
    "            ## Hacemos como con L, pero usando directamente R para poder mantener siempre la misma matriz de modularidad\n",
    "            Rp = R[v >= 0][:, v >= 0] # Parte de R asociada a los valores positivos de v\n",
    "            Rm = R[v < 0][:, v < 0] # Parte asociada a los valores negativos de v\n",
    "            vp,lp = metpot1(Rp)  # autovector principal de Rp\n",
    "            vm,lm = metpot1(Rm) # autovector principal de Rm\n",
    "\n",
    "            # Calculamos el cambio en Q que se produciría al hacer esta partición\n",
    "            Q1 = 0\n",
    "            if not all(vp>0) or all(vp<0):\n",
    "               Q1 = np.sum(Rp[vp>0,:][:,vp>0]) + np.sum(Rp[vp<0,:][:,vp<0])\n",
    "            if not all(vm>0) or all(vm<0):\n",
    "                Q1 += np.sum(Rm[vm>0,:][:,vm>0]) + np.sum(Rm[vm<0,:][:,vm<0])\n",
    "            if Q0 >= Q1: # Si al partir obtuvimos un Q menor, devolvemos la última partición que hicimos\n",
    "                return([[ni for ni,vi in zip(nombres_s,v) if vi>0],[ni for ni,vi in zip(nombres_s,v) if vi<0]])\n",
    "            else:\n",
    "                # Sino, repetimos para los subniveles\n",
    "                return(modularidad_iterativo(A, Rp,\n",
    "                                     nombres_s=[ni for ni,vi in zip(nombres_s,v) if vi>0]) +\n",
    "                modularidad_iterativo(A, Rm,\n",
    "                                     nombres_s=[ni for ni,vi in zip(nombres_s,v) if vi<0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 4.\n",
    "\n",
    "> Usando la red de museos definida en el TP anterior,\n",
    "calcule las particiones óptimas usando el método basado en el laplaciano y el método\n",
    "basado en la modularidad. Utilice la matriz de adyacencia A construida usando $m$ = 3, 5, 10, 50, luego de haberla simetrizado haciendo 5 \n",
    ">$$\n",
    " A' = \\frac{1}{2}(A + A^t)\n",
    ">$$\n",
    ">con $A'$ la versión simetrizada de A y $⌈x⌉$ el resultado de aplicar la funcion ceiling a $x$.\n",
    "Exploren cómo cambia la estructura de comunidades obtenida usando la modularidad (en términos de número de comunidades, de su tamaño y de las regiones del mapa que ocupan, así como la estabilidad ante realizaciones) encontrada para distintos valores de $m$. Comparen visualmente las comunidades obtenidas mediante el laplaciano y la modularidad, buscando un número de niveles que dé una cantidad de comunidades comparable en ambos métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ploteadorColor(A, colors, matplot_ax=None):\n",
    "    G_m = nx.from_numpy_array(A) # Construimos la red a partir de la matriz de adyacencia\n",
    "    G_m_layout = {i:v for i,v in enumerate(zip(museos.to_crs(\"EPSG:22184\").get_coordinates()['x'],museos.to_crs(\"EPSG:22184\").get_coordinates()['y']))}\n",
    "\n",
    "    node_community = {}\n",
    "    for i, comunidad in enumerate(colors):\n",
    "        for nodo in comunidad:\n",
    "            node_community[nodo] = i\n",
    "\n",
    "    node_colors = [node_community[n] for n in G_m.nodes()]\n",
    "    \n",
    "    barrios.to_crs(\"EPSG:22184\").boundary.plot(color='gray',ax=matplot_ax) # Graficamos Los barrios\n",
    "\n",
    "    nx.draw_networkx(G_m, G_m_layout, with_labels=False, node_size=50, node_color=node_colors, ax=matplot_ax) # Graficamos red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simetrizar(A):\n",
    "    return np.ceil((A + A.T)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLaplaciano(A, ax, niveles = 1, title=None):\n",
    "    ploteadorColor(A, laplaciano_iterativo(A, niveles), ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "def plotModularizado(A, ax, title=None):\n",
    "    ploteadorColor(A, modularidad_iterativo(A, calcula_R(A)), ax)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_m3 = simetrizar(construye_adyacencia(D, 3))\n",
    "A_m5 = simetrizar(construye_adyacencia(D, 5))\n",
    "A_m10 = simetrizar(construye_adyacencia(D, 10))\n",
    "A_m50 = simetrizar(construye_adyacencia(D, 50))\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(14, 20), tight_layout=False)\n",
    "plotLaplaciano(A_m3, axs[0, 0], 3, \"Corte laplaciano - m3\")\n",
    "plotModularizado(A_m3, axs[0, 1], \"Modularidad - m3\")\n",
    "\n",
    "plotLaplaciano(A_m5, axs[1, 0], 3, \"Corte laplaciano - m5\")\n",
    "plotModularizado(A_m5, axs[1, 1], \"Modularidad - m5\")\n",
    "\n",
    "plotLaplaciano(A_m10, axs[2, 0], 2, \"Corte laplaciano - m10\")\n",
    "plotModularizado(A_m10, axs[2, 1], \"Modularidad - m10\")\n",
    "\n",
    "plotLaplaciano(A_m50, axs[3, 0], 1, \"Corte laplaciano - m50\")\n",
    "plotModularizado(A_m50, axs[3, 1], \"Modularidad - m50\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos a partir de los distintos resultados obtenidos que en el metodo de la modularidad la cantidad de cortes disminuye a medida que aumentan la cantidad de adyacencias. En el metodo del laplaciano, dada una cantidad de cortes acorde, vemos agrupaciones similares a las del metodo de la modularidad\n",
    "Por ejemplo, cuando la cantidad de adyacencias es baja, hay mayor diferencia en la conformacion de los grupos entre los distintos metodos, no asi en la cantidad de grupos. Ademas estan distribuidos a lo largo del mapa por zonas que se podrian considerar \"intuitivas\".\n",
    "A medida que aumentan las adyacencias aumenta la similitud en los grupos obtenidos con ambos metodos y se vuelve mas dificil la distincion a simple vista del porque de la conformacion de los grupos. Ya con la cantidad mas grande de adyacencias ambos grupos son iguales independientemente del metodo y obtenemos una clara distincion entre museos ubicados en el Sureste y museos ubicados en el Noroeste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sintesis Final\n",
    "En ambos trabajos pudimos observar influencias y extraer informacion a partir de un dataset de museos utilizando diferentes herramientas del algebra lineal; Estas nos permitieron ver como se ven afectados los comportamientos del grafo a partir de la variacion de ciertos parametros. \n",
    "\n",
    "Lo que pudimos aprender de los metodos utilizados en la primer parte es que el PageRank es altamente sensible a la eleccion de parametros variables que no dependen del dataset como puede ser la cantidad de adyacencias (m) o la aleatoriedad del surfer (alpha), entonces para obtener un analisis util hay que considerar las distintas combinaciones de las variables y elegir las adecuadas para obtener el resultado mas fidedigno, un ejemplo de esto seria que si se mantienen fijas las adyacencias y se aumenta la aleatoriedad del \"surfer\" se hace mas heterogenea la reparticion de visitantes entre museos, aunque tiende a conservar el orden de los rankings (quien estaba primero con un alpha bajo sigue estando en los primeros puestos con un alpha alto). Por lo que podemos ver que el PageRank, aunque util, tiene dependencia de valores arbitrarios que no dependen del dataset y que quedan a eleccion de la persona que realiza el estudio, por lo que es dificil de conseguir un PageRank \"ideal\".\n",
    "\n",
    "En cuanto a lo que pudimos aprender en el TP2 es que el metodo de modularizacion, el cual sirve para distinguir clusters en un dataset, es sensible a la cantidad de adyacencias. Un ejemplo de esto seria que a medida que aumentan las adyacencias, el metodo de subdivision por modularidad genera menor cantidad de grupos; Y en el metodo del laplaciano cuando se elige una cantidad de cortes acorde a la de la modularidad, la distribucion de los grupos en ambos es similar. Esto tiene sentido ya que el objetivo de ambos metodos es poder obtener los clusters de la matriz de adyacencia, lo cual nos sirve para poder distinguir bajo que conjuntos se suele mantener la gente a medida que se trasladan de museo. \n",
    "\n",
    "En sintesis, ambos trabajos practicos nos otorgaron las herramientas necesarias para poder extraer informacion y sacar conclusiones a partir de un dataset utilizando metodos matriciales, aunque estos metodos no sean infalibles y sean sensibles o cambien a partir de la eleccion arbitraria de parametros. Por lo que para poder llevar a cabo un analisis exhaustivo y poder sacar conclusiones deberiamos considerar un rango amplio de valores para los parametros de los distintos metodos, tanto para observar como estos afectan al resultado como para no sacar conclusiones erroneas a partir de valores arbitrarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
